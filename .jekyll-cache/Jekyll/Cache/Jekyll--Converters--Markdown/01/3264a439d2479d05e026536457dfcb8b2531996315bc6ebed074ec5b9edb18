I"vV<p><img src="/images/coredns.png" alt="coreDNS logo" /></p>

<p>A Kubernetes cluster is isolated by default.  The pods may (or may not) be using an overlay network IP scheme, the Cluster IPs may be using yet another CIDR block, and DNS is entirely local to the cluster.  So, how pray tell, do you interact with such an isolated beast?</p>

<p>Well, for <em>services</em> you use Cloud load-balancers which direct traffic to the pods indirectly via NodePorts.  For DNS, one typically uses the <a href="https://github.com/kubernetes-sigs/external-dns">External DNS</a> tool to export local Kubernetes names to global DNS names.</p>

<p><img src="/images/external-dns.png" alt="external DNS logo" /></p>

<p>Still, this does not solve all your DNS issues. External DNS only <strong>exports</strong> you name information.  What if you want to <strong>import</strong> information, say from a private zone (thinking AWS here)?!</p>

<p>Well, fortunately - there is a solution!  <a href="https://kubernetes.io/blog/2018/07/10/coredns-ga-for-kubernetes-cluster-dns/">CoreDNS</a>, the default DNS provider for Kubernetes (it replaced KubeDNS in version 1.11) has a route53 plug-in. This plug-in will periodically poll your local Route53 private or public zones, and make all entries locally available to your Kubernetes/EKS cluster. This is very useful if your cluster needs to access resources within your VPC, which are private.</p>

<p>The other option is to use AWS <a href="https://aws.amazon.com/cloud-map/">CloudMap</a>. Cloud Map provides service discovery for compute resources within your VPC. Check out this <a href="https://aws.amazon.com/blogs/containers/cross-amazon-eks-cluster-app-mesh-using-aws-cloud-map/">blog</a> post, which leveraged it for EKS to EKS communication using App Mesh (a type of servive mesh).  Pretty cool stuff!  :-)</p>

<h2 id="configuring-coredns">Configuring CoreDNS</h2>
<p>The settings for CoreDNS are handled via a configmap, which lives in the kube-system namespace.  We must edit this CM in order to change things.</p>

<h3 id="original-configmap">Original ConfigMap</h3>
<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
</code></pre></div></div>

<h3 id="new-configmap">New ConfigMap</h3>
<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">data</span><span class="pi">:</span>
  <span class="na">Corefile</span><span class="pi">:</span> <span class="pi">|</span>
    <span class="s">nick.test:53 {</span>
      <span class="s">route53 nick.test.:Z0699699UVAMCRGMLWSE {</span>
        <span class="s">aws_access_key AKIA6NSNI3FKVFZPXRYK pCjWx3Kg24ChgqhwUvDefkfc3juMBmlwZ/FZx04k</span>
      <span class="s">}</span>
    <span class="s">}</span>
    <span class="s">.:53 {</span>
        <span class="s">errors</span>
        <span class="s">health</span>
        <span class="s">kubernetes cluster.local in-addr.arpa ip6.arpa {</span>
          <span class="s">pods insecure</span>
          <span class="s">upstream</span>
          <span class="s">fallthrough in-addr.arpa ip6.arpa</span>
        <span class="s">}</span>
        <span class="s">prometheus :9153</span>
        <span class="s">forward . /etc/resolv.conf</span>
        <span class="s">cache 30</span>
        <span class="s">loop</span>
        <span class="s">reload</span>
        <span class="s">loadbalance</span>
    <span class="s">}</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">ConfigMap</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">annotations</span><span class="pi">:</span>
    <span class="s">kubectl.kubernetes.io/last-applied-configuration</span><span class="pi">:</span> <span class="pi">|</span>
      <span class="s">{"apiVersion":"v1","data":{"Corefile":".:53 {\n    errors\n    health\n    kubernetes cluster.local in-addr.arpa ip6.arpa {\n      pods insecure\n      upstream\n      fallthrough in-addr.arpa ip6.arpa\n    }\n    prometheus :9153\n    forward . /etc/resolv.conf\n    cache 30\n    loop\n    reload\n    loadbalance\n}\n"},"kind":"ConfigMap","metadata":{"annotations":{},"labels":{"eks.amazonaws.com/component":"coredns","k8s-app":"kube-dns"},"name":"coredns","namespace":"kube-system"}}</span>
  <span class="na">creationTimestamp</span><span class="pi">:</span> <span class="s2">"</span><span class="s">2019-11-19T22:11:56Z"</span>
  <span class="na">labels</span><span class="pi">:</span>
    <span class="s">eks.amazonaws.com/component</span><span class="pi">:</span> <span class="s">coredns</span>
    <span class="na">k8s-app</span><span class="pi">:</span> <span class="s">kube-dns</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">coredns</span>
  <span class="na">namespace</span><span class="pi">:</span> <span class="s">kube-system</span>
  <span class="na">resourceVersion</span><span class="pi">:</span> <span class="s2">"</span><span class="s">153512"</span>
  <span class="na">selfLink</span><span class="pi">:</span> <span class="s">/api/v1/namespaces/kube-system/configmaps/coredns</span>
  <span class="na">uid</span><span class="pi">:</span> <span class="s">997034b1-0b19-11ea-89c2-06bc2ec33374</span>
</code></pre></div></div>

<h2 id="create-nginx-task-in-ecs">Create nginx task in ECS</h2>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ fargate task run nginx-dns-test -i nginx:latest --security-group-id sg-097c56c9dfd62802a --region us-west-2

$ fargate task ps nginx-dns-test --region us-west-2
ID					IMAGE		STATUS	RUNNING	IP	CPU	MEMORY	
fb5f875a91904a3a88be2d24ed04b37f	nginx:latest	running	2m25s	172.31.52.184	256	512
</code></pre></div></div>
<p><em>Note: I changed the public IP to the private one for clarity</em></p>

<h2 id="create-route53-private-domain">Create Route53 private domain</h2>
<p>I created a private domain for <em>nick.test</em>. I then created an <strong>A</strong> record, pointing at the private IP address of the Farate container I just created.</p>

<p><img src="/images/zone1.png" alt="Route53 zone" />
<img src="/images/route532.png" alt="Zone details" /></p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>kubectl run dnstools <span class="nt">--generator</span><span class="o">=</span>run-pod/v1 <span class="nt">--image</span><span class="o">=</span>infoblox/dnstools <span class="nt">-it</span>


<span class="nv">$ </span>kubectl get pods <span class="nt">-o</span> wide
NAME                                READY   STATUS    RESTARTS   AGE   IP
nginx-deployment-76bf4969df-m4szq   1/1     Running   0          73s   172.31.31.49
nginx-deployment-76bf4969df-zfsjn   1/1     Running   0          73s   172.31.13.159
</code></pre></div></div>

<h2 id="now-lets-look-at-the-node-labels">Now let’s look at the node labels</h2>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>NAME                                          STATUS   ROLES    AGE    VERSION              LABELS
ip-172-31-29-212.us-west-2.compute.internal   Ready    &lt;none&gt;   109m   v1.13.7-eks-c57ff8   alpha.eksctl.io/cluster-name<span class="o">=</span>cluster0,alpha.eksctl.io/instance-id<span class="o">=</span>i-008eeb529825fa431,alpha.eksctl.io/nodegroup-name<span class="o">=</span>ng-1,beta.kubernetes.io/arch<span class="o">=</span>amd64,beta.kubernetes.io/instance-type<span class="o">=</span>m5.xlarge,beta.kubernetes.io/os<span class="o">=</span>linux,failure-domain.beta.kubernetes.io/region<span class="o">=</span>us-west-2,failure-domain.beta.kubernetes.io/zone<span class="o">=</span>us-west-2b,kubernetes.io/hostname<span class="o">=</span>ip-172-31-29-212.us-west-2.compute.internal
ip-172-31-6-222.us-west-2.compute.internal    Ready    &lt;none&gt;   109m   v1.13.7-eks-c57ff8   alpha.eksctl.io/cluster-name<span class="o">=</span>cluster0,alpha.eksctl.io/instance-id<span class="o">=</span>i-02ae95c403058e42f,alpha.eksctl.io/nodegroup-name<span class="o">=</span>ng-1,beta.kubernetes.io/arch<span class="o">=</span>amd64,beta.kubernetes.io/instance-type<span class="o">=</span>m5.xlarge,beta.kubernetes.io/os<span class="o">=</span>linux,failure-domain.beta.kubernetes.io/region<span class="o">=</span>us-west-2,failure-domain.beta.kubernetes.io/zone<span class="o">=</span>us-west-2c,kubernetes.io/hostname<span class="o">=</span>ip-172-31-6-222.us-west-2.compute.internal
</code></pre></div></div>

<p>If you look carefully, you will see the following labels associated with the nodes:</p>

<hr />

<table>
  <tbody>
    <tr>
      <td><strong>LABEL</strong></td>
      <td><strong>region</strong> or <strong>zone</strong></td>
    </tr>
    <tr>
      <td>failure-domain.beta.kubernetes.io/<strong>region</strong></td>
      <td><em>us-west-2</em></td>
    </tr>
    <tr>
      <td>failure-domain.beta.kubernetes.io/<strong>zone</strong></td>
      <td><em>us-west-2c</em></td>
    </tr>
  </tbody>
</table>

<hr />

<h2 id="using-a-custom-scheduler">Using a custom scheduler</h2>
<p>Now, even though I explained how good the default scheduler is, and even demonstrated that it is zone aware, it is not perfect.  For example. let’s say I have two node-groups.  One group is on-demand instances, and another node-group consists of Spot instances.  Perhaps I prefer for the scheduler to use the Spot instances first.  If they are filled, only then use the on-demand instances. This is actually quite tricky to do using the default scheduler. One way is to use an open-source project called <a href="https://github.com/pusher/k8s-spot-rescheduler">k8-spot-scheduler</a>. Another project worth mentioning is a <a href="https://github.com/kubernetes-incubator/descheduler"><em>descheduler</em></a>. It allows for pods to be moved around if certain rules are violated (i.e. too many pods on the same host).</p>

<p>Another way to accomplish this goal is to create your own custom scheduler, with the logic tuned to your requirements. We will create a crude scheduler in order to demonstrate how easy this really is. The rest of this code is based upon a talk by Kelsey Hightower and his <a href="https://skillsmatter.com/skillscasts/7897-keynote-get-your-ship-together-containers-are-here-to-stay">talk</a>, from 2016.</p>

<p>We are going to write our new scheduleder in Elixir, based upon this <a href="http://agonzalezro.github.io/scheduling-your-kubernetes-pods-with-elixir.html">blog</a> and this <a href="https://github.com/agonzalezro/escheduler">code</a>. I have updated the <a href="https://github.com/nbrandaleone/escheduler">code</a>, and posted it on Github. Why <a href="https://elixir-lang.org/">elixir</a>? I like the power of functional programming, the sytax of Ruby, and the ability to create 100,000 of processes easily. It would not be a bad choice for a real scheduler, if you were so inclined. Of course, you can peruse the actual <a href="https://github.com/kubernetes/kubernetes/blob/master/pkg/scheduler/scheduler.go">Go</a> code of the default scheduler as well.</p>

<h2 id="find-all-pods-that-are-unscheduled-and-assigned-to-our-scheduler">Find all pods that are unscheduled, and assigned to our scheduler</h2>

<p><strong>Elixir code</strong></p>

<figure class="highlight"><pre><code class="language-elixir" data-lang="elixir"><span class="k">def</span> <span class="n">unscheduled_pods</span><span class="p">()</span> <span class="k">do</span>
    <span class="n">is_managed_by_us</span> <span class="o">=</span> <span class="o">&amp;</span><span class="p">(</span><span class="n">get_in</span><span class="p">(</span><span class="nv">&amp;1</span><span class="p">,</span> <span class="p">[</span><span class="s2">"spec"</span><span class="p">,</span> <span class="s2">"schedulerName"</span><span class="p">])</span> <span class="o">==</span> <span class="nv">@name</span><span class="p">)</span>

    <span class="n">resp</span> <span class="o">=</span> <span class="no">HTTPoison</span><span class="o">.</span><span class="n">get!</span> <span class="s2">"http://127.0.0.1:8001/api/v1/pods?fieldSelector=spec.nodeName="</span>
    <span class="n">resp</span><span class="o">.</span><span class="n">body</span>
    <span class="o">|&gt;</span> <span class="no">Poison</span><span class="o">.</span><span class="n">decode!</span>
    <span class="o">|&gt;</span> <span class="n">get_in</span><span class="p">([</span><span class="s2">"items"</span><span class="p">])</span>
    <span class="o">|&gt;</span> <span class="no">Enum</span><span class="o">.</span><span class="n">filter</span><span class="p">(</span><span class="n">is_managed_by_us</span><span class="p">)</span>
    <span class="o">|&gt;</span> <span class="no">Enum</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="o">&amp;</span><span class="p">(</span><span class="n">get_in</span><span class="p">(</span><span class="nv">&amp;1</span><span class="p">,</span> <span class="p">[</span><span class="s2">"metadata"</span><span class="p">,</span> <span class="s2">"name"</span><span class="p">])))</span>
<span class="k">end</span></code></pre></figure>

<p>As you can see, we are using <code class="highlighter-rouge">127.0.0.1:8001</code> to query our API. This is possible thanks to the <code class="highlighter-rouge">kubectl proxy</code> command which easily allows us to test out our new scheduler without first converting it to a pod, and worrying about Service Accounts and other details.</p>

<h2 id="get-a-list-of-available-nodes">Get a list of available nodes</h2>
<p>This function simply returns a list of available nodes. We are not checking to see if the nodes have any spare capacity - just that they are available.</p>

<figure class="highlight"><pre><code class="language-elixir" data-lang="elixir"><span class="k">def</span> <span class="n">nodes</span><span class="p">()</span> <span class="k">do</span>
    <span class="n">resp</span> <span class="o">=</span> <span class="no">HTTPoison</span><span class="o">.</span><span class="n">get!</span> <span class="s2">"http://127.0.0.1:8001/api/v1/nodes"</span>
    <span class="n">resp</span><span class="o">.</span><span class="n">body</span>
    <span class="o">|&gt;</span> <span class="no">Poison</span><span class="o">.</span><span class="n">decode!</span>
    <span class="o">|&gt;</span> <span class="n">get_in</span><span class="p">([</span><span class="s2">"items"</span><span class="p">])</span>
    <span class="o">|&gt;</span> <span class="no">Enum</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="o">&amp;</span><span class="p">(</span><span class="n">get_in</span><span class="p">(</span><span class="nv">&amp;1</span><span class="p">,</span> <span class="p">[</span><span class="s2">"metadata"</span><span class="p">,</span> <span class="s2">"name"</span><span class="p">])))</span>
<span class="k">end</span></code></pre></figure>

<h2 id="the-bind-function">The Bind Function</h2>
<p>Once we have a list of unscheduled pods, and potential nodes to run them on - we must bind the pods to a node. We call the bind endpoint, like this:</p>

<figure class="highlight"><pre><code class="language-elixir" data-lang="elixir"><span class="k">def</span> <span class="n">bind</span><span class="p">(</span><span class="n">pod_name</span><span class="p">,</span> <span class="n">node_name</span><span class="p">)</span> <span class="k">do</span>
    <span class="n">url</span> <span class="o">=</span> <span class="s2">"http://127.0.0.1:8001/api/v1/namespaces/default/pods/</span><span class="si">#{</span><span class="n">pod_name</span><span class="si">}</span><span class="s2">/binding"</span>
    <span class="n">body</span> <span class="o">=</span> <span class="no">Poison</span><span class="o">.</span><span class="n">encode!</span><span class="p">(%{</span>
      <span class="ss">apiVersion:</span> <span class="s2">"v1"</span><span class="p">,</span>
      <span class="ss">kind:</span> <span class="s2">"Binding"</span><span class="p">,</span>
      <span class="ss">metadata:</span> <span class="p">%{</span>
        <span class="ss">name:</span> <span class="n">pod_name</span>
      <span class="p">},</span>
      <span class="ss">target:</span> <span class="p">%{</span>
        <span class="ss">apiVersion:</span> <span class="s2">"v1"</span><span class="p">,</span>
        <span class="ss">kind:</span> <span class="s2">"Node"</span><span class="p">,</span>
        <span class="ss">name:</span> <span class="n">node_name</span>
      <span class="p">}</span>
    <span class="p">})</span>
    <span class="n">headers</span> <span class="o">=</span> <span class="p">[{</span><span class="s2">"Content-Type"</span><span class="p">,</span> <span class="s2">"application/json"</span><span class="p">}]</span>
    <span class="n">options</span> <span class="o">=</span> <span class="p">[</span><span class="ss">follow_redirect:</span> <span class="no">true</span><span class="p">]</span>

    <span class="no">HTTPoison</span><span class="o">.</span><span class="n">post!</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">body</span><span class="p">,</span> <span class="n">headers</span><span class="p">,</span> <span class="n">options</span><span class="p">)</span>
    <span class="no">IO</span><span class="o">.</span><span class="n">puts</span> <span class="s2">"</span><span class="si">#{</span><span class="n">pod_name</span><span class="si">}</span><span class="s2"> pod scheduled in </span><span class="si">#{</span><span class="n">node_name</span><span class="si">}</span><span class="s2">"</span>
<span class="k">end</span></code></pre></figure>

<h2 id="scheduling-strategy">Scheduling Strategy</h2>
<p>For our simple scheduler, we will go with a random scheduling strategy. However, this is where one could provide significant business value by altering the default strategy, in order to take into account Spot prices, for example.</p>

<figure class="highlight"><pre><code class="language-elixir" data-lang="elixir"><span class="k">def</span> <span class="n">schedule</span><span class="p">(</span><span class="n">pods</span><span class="p">)</span> <span class="k">do</span>
    <span class="n">pods</span>
    <span class="o">|&gt;</span> <span class="no">Enum</span><span class="o">.</span><span class="n">each</span><span class="p">(</span><span class="o">&amp;</span><span class="p">(</span><span class="n">bind</span><span class="p">(</span><span class="nv">&amp;1</span><span class="p">,</span> <span class="no">Enum</span><span class="o">.</span><span class="n">random</span><span class="p">(</span><span class="n">nodes</span><span class="p">()))))</span>
<span class="k">end</span></code></pre></figure>

<h2 id="test-it-out">Test it out</h2>
<ol>
  <li>Create some pods or a Deployment with the spec having the following addition: <code class="highlighter-rouge">schedulerName: escheduler</code>.</li>
  <li>Start up the <em>proxy</em>.</li>
  <li>Run your custom scheduler: <code class="highlighter-rouge">$ ./escheduler</code></li>
</ol>

<h3 id="before-the-scheduler-is-run-the-pods-are-pending">Before the scheduler is run, the pods are <em>Pending</em></h3>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>NAME                     READY   STATUS    RESTARTS   AGE
nginx-7d96855ffb-947n8   0/1     Pending   0          11s
nginx-7d96855ffb-qnhgh   0/1     Pending   0          11s
nginx-7d96855ffb-sbltl   0/1     Pending   0          11s
</code></pre></div></div>

<h3 id="afterwards">Afterwards…</h3>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>NAME                     READY   STATUS    RESTARTS   AGE
nginx-7d96855ffb-947n8   1/1     Running   0          31s
nginx-7d96855ffb-qnhgh   1/1     Running   0          31s
nginx-7d96855ffb-sbltl   1/1     Running   0          31s
</code></pre></div></div>

<h2 id="the-future-of-custom-schedulers">The future of custom schedulers</h2>
<p>As cool as it is to create your own scheduler, it is unlikely you could create a production quality one without a great deal of effort. Therefore, it is exciting news that in Kubernetes 1.15+ (alpha) that it will be possible to add extensions into the default Kubernetes scheduler.  This will truely be the best of all worlds, where we can rely upon a rock-solid scheduler, but extend it to meet our business needs.  Please read about it <a href="https://kubernetes.io/docs/concepts/configuration/scheduling-framework/">here</a></p>

<p><img src="/images/scheduling-framework-extensions.png" alt="Scheduler Extension" /></p>

<hr />

<h3 id="references">References</h3>
<ul>
  <li><a href="https://kubernetes.io/docs/concepts/scheduling/kube-scheduler/">Kubernetes Scheduler</a></li>
  <li><a href="https://kubernetes.io/docs/concepts/scheduling/scheduler-perf-tuning/">Scheduler Performance Tuning</a></li>
  <li><a href="https://jvns.ca/blog/2017/07/27/how-does-the-kubernetes-scheduler-work/">Julia Evans - How the Kubernetes scheduler work?</a></li>
  <li><a href="https://banzaicloud.com/blog/k8s-custom-scheduler/">Writing custom Kubernetes schedulers</a></li>
  <li><a href="https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/">Configure Multiple Schedulers</a></li>
  <li><a href="https://sysdig.com/blog/kubernetes-scheduler/">Writing a custom Kubernetes scheduler using monitoring metrics</a></li>
  <li><a href="https://thenewstack.io/implementing-advanced-scheduling-techniques-with-kubernetes/">Implementing Advanced Scheduling Techniques with Kubernetes</a></li>
  <li><a href="https://kubernetes.io/docs/concepts/configuration/scheduling-framework/">Scheduling Framework</a></li>
  <li><a href="https://github.com/kelseyhightower/scheduler/blob/master/kubernetes.go">Kelsey’s toy scheduler</a></li>
</ul>
:ET