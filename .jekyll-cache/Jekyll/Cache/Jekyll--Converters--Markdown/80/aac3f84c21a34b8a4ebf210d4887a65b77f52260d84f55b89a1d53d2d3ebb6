I"ñ,<p><img src="/images/coredns.png" alt="coreDNS logo" /></p>

<p>A Kubernetes cluster is isolated by default.  The pods may (or may not) be using an overlay network IP scheme, the Cluster IPs may be using yet another CIDR block, and DNS is entirely local to the cluster.  So, how pray tell, do you interact with such an isolated beast?</p>

<p>Well, for <em>services</em> you use Cloud load-balancers which direct traffic to the pods indirectly via NodePorts.  For DNS, one typically uses the <a href="https://github.com/kubernetes-sigs/external-dns">External DNS</a> tool to export local Kubernetes names to global DNS names.</p>

<p><img src="/images/external-dns.png" alt="external DNS logo" /></p>

<p>Still, this does not solve all your DNS issues. External DNS only <strong>exports</strong> you name information.  What if you want to <strong>import</strong> information, say from a private zone (thinking AWS here)?!</p>

<p>Well, fortunately - there is a solution!  <a href="https://kubernetes.io/blog/2018/07/10/coredns-ga-for-kubernetes-cluster-dns/">CoreDNS</a>, the default DNS provider for Kubernetes (it replaced KubeDNS in version 1.11) has a route53 plug-in. This plug-in will periodically poll your local Route53 private or public zones, and make all entries locally available to your Kubernetes/EKS cluster. This is very useful if your cluster needs to access resources within your VPC, which are private.</p>

<p>The other option is to use AWS <a href="https://aws.amazon.com/cloud-map/">CloudMap</a>. Cloud Map provides service discovery for compute resources within your VPC. Check out this <a href="https://aws.amazon.com/blogs/containers/cross-amazon-eks-cluster-app-mesh-using-aws-cloud-map/">blog</a> post, which leveraged it for EKS to EKS communication using App Mesh (a type of servive mesh).  Pretty cool stuff!  :-)</p>

<h2 id="configuring-coredns">Configuring CoreDNS</h2>
<p>The settings for CoreDNS are handled via a configmap, which lives in the kube-system namespace.  We must edit this CM in order to add the route53 plugin.0:w</p>

<h3 id="original-configmap">Original ConfigMap</h3>
<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">data</span><span class="pi">:</span>
  <span class="na">Corefile</span><span class="pi">:</span> <span class="pi">|</span>
    <span class="s">.:53 {</span>
        <span class="s">errors</span>
        <span class="s">health</span>
        <span class="s">kubernetes cluster.local in-addr.arpa ip6.arpa {</span>
          <span class="s">pods insecure</span>
          <span class="s">upstream</span>
          <span class="s">fallthrough in-addr.arpa ip6.arpa</span>
        <span class="s">}</span>
        <span class="s">prometheus :9153</span>
        <span class="s">forward . /etc/resolv.conf</span>
        <span class="s">cache 30</span>
        <span class="s">loop</span>
        <span class="s">reload</span>
        <span class="s">loadbalance</span>
    <span class="s">}</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">ConfigMap</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">annotations</span><span class="pi">:</span>
    <span class="s">kubectl.kubernetes.io/last-applied-configuration</span><span class="pi">:</span> <span class="pi">|</span>
      <span class="s">{"apiVersion":"v1","data":{"Corefile":".:53 {\n    errors\n    health\n    kubernetes cluster.local in-addr.arpa ip6.arpa {\n      pods insecure\n      upstream\n      fallthrough in-addr.arpa ip6.arpa\n    }\n    prometheus :9153\n    forward . /etc/resolv.conf\n    cache 30\n    loop\n    reload\n    loadbalance\n}\n"},"kind":"ConfigMap","metadata":{"annotations":{},"labels":{"eks.amazonaws.com/component":"coredns","k8s-app":"kube-dns"},"name":"coredns","namespace":"kube-system"}}</span>
  <span class="na">creationTimestamp</span><span class="pi">:</span> <span class="s2">"</span><span class="s">2019-11-19T22:11:56Z"</span>
  <span class="na">labels</span><span class="pi">:</span>
    <span class="s">eks.amazonaws.com/component</span><span class="pi">:</span> <span class="s">coredns</span>
    <span class="na">k8s-app</span><span class="pi">:</span> <span class="s">kube-dns</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">coredns</span>
  <span class="na">namespace</span><span class="pi">:</span> <span class="s">kube-system</span>
  <span class="na">resourceVersion</span><span class="pi">:</span> <span class="s2">"</span><span class="s">140889"</span>
  <span class="na">selfLink</span><span class="pi">:</span> <span class="s">/api/v1/namespaces/kube-system/configmaps/coredns</span>
  <span class="na">uid</span><span class="pi">:</span> <span class="s">997034b1-0b19-11ea-89c2-06bc2ec33374</span>
</code></pre></div></div>

<h3 id="new-configmap">New ConfigMap</h3>
<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">data</span><span class="pi">:</span>
  <span class="na">Corefile</span><span class="pi">:</span> <span class="pi">|</span>
    <span class="s">nick.test:53 {</span>
      <span class="s">route53 nick.test.:Z0699699UVAMCRGMLWSE {</span>
      <span class="s">aws_access_key &lt;access_key&gt; &lt;secret accress key&gt;</span>
      <span class="s">}</span>
    <span class="s">}</span>
    <span class="s">.:53 {</span>
        <span class="s">errors</span>
        <span class="s">health</span>
        <span class="s">kubernetes cluster.local in-addr.arpa ip6.arpa {</span>
          <span class="s">pods insecure</span>
          <span class="s">upstream</span>
          <span class="s">fallthrough in-addr.arpa ip6.arpa</span>
        <span class="s">}</span>
        <span class="s">prometheus :9153</span>
        <span class="s">forward . /etc/resolv.conf</span>
        <span class="s">cache 30</span>
        <span class="s">loop</span>
        <span class="s">reload</span>
        <span class="s">loadbalance</span>
    <span class="s">}</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">ConfigMap</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">annotations</span><span class="pi">:</span>
    <span class="s">kubectl.kubernetes.io/last-applied-configuration</span><span class="pi">:</span> <span class="pi">|</span>
      <span class="s">{"apiVersion":"v1","data":{"Corefile":".:53 {\n    errors\n    health\n    kubernetes cluster.local in-addr.arpa ip6.arpa {\n      pods insecure\n      upstream\n      fallthrough in-addr.arpa ip6.arpa\n    }\n    prometheus :9153\n    forward . /etc/resolv.conf\n    cache 30\n    loop\n    reload\n    loadbalance\n}\n"},"kind":"ConfigMap","metadata":{"annotations":{},"labels":{"eks.amazonaws.com/component":"coredns","k8s-app":"kube-dns"},"name":"coredns","namespace":"kube-system"}}</span>
  <span class="na">creationTimestamp</span><span class="pi">:</span> <span class="s2">"</span><span class="s">2019-11-19T22:11:56Z"</span>
  <span class="na">labels</span><span class="pi">:</span>
    <span class="s">eks.amazonaws.com/component</span><span class="pi">:</span> <span class="s">coredns</span>
    <span class="na">k8s-app</span><span class="pi">:</span> <span class="s">kube-dns</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">coredns</span>
  <span class="na">namespace</span><span class="pi">:</span> <span class="s">kube-system</span>
  <span class="na">resourceVersion</span><span class="pi">:</span> <span class="s2">"</span><span class="s">153512"</span>
  <span class="na">selfLink</span><span class="pi">:</span> <span class="s">/api/v1/namespaces/kube-system/configmaps/coredns</span>
  <span class="na">uid</span><span class="pi">:</span> <span class="s">997034b1-0b19-11ea-89c2-06bc2ec33374</span>
</code></pre></div></div>

<p>You may have noticed that I had to put an access key/secret access key into the configuration.  The manuals states that you should be able to use the default credential provider available to the node (or perhaps IRSA), but I was not able to get it working in the small amount of time I dedicated to this project.</p>

<h2 id="create-a-fargate-task-in-ecs">Create a fargate task in ECS</h2>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ fargate task run nginx-dns-test -i nginx:latest --security-group-id sg-097c56c9dfd62802a --region us-west-2

$ fargate task ps nginx-dns-test --region us-west-2
ID					IMAGE		STATUS	RUNNING	IP	        CPU	MEMORY	
fb5f875a91904a3a88be2d24ed04b37f	nginx:latest	running	2m25s	172.31.52.184	256	512
</code></pre></div></div>
<p><em>Note: I changed the public IP to the private one for clarity</em></p>

<p><img src="/images/route532.png" alt="Zone details" />
k</p>
<h2 id="create-a-route53-private-domain">Create a Route53 private domain</h2>
<p>I created a private domain for <em>nick.test</em>. I then created an <strong>A</strong> record for the fargate task I just created, calling the record <em>nginx.nick.test</em>.</p>

<p><img src="/images/zone1.png" alt="Route53 zone" /></p>

<h2 id="verify-if-the-eks-cluster-can-resolve-the-private-route53-zone">Verify if the EKS cluster can resolve the private Route53 zone</h2>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>kubectl run dnstools <span class="nt">--generator</span><span class="o">=</span>run-pod/v1 <span class="nt">--image</span><span class="o">=</span>infoblox/dnstools <span class="nt">-it</span>

<span class="nv">$ </span>dig <span class="nt">-t</span> a nginx.nick.test. 
<span class="p">;</span> &lt;&lt;<span class="o">&gt;&gt;</span> DiG 9.8.3-P1
<span class="p">;;</span> global options: +cmd
<span class="p">;;</span> Got answer:
<span class="p">;;</span> -&gt;&gt;HEADER<span class="o">&lt;&lt;-</span> <span class="no">opcode</span><span class="sh">: QUERY, status: NOERROR, id: 60796
;; flags: qr aa rd; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 0
...
;nginx.nick.test.            IN    A
;; ANSWER SECTION:
nginx.nick.test.        3600    IN    A    172.31.52.184
...
</span></code></pre></div></div>

<p>Succcess!!  I have shown that it is possible to import Route53 zones into your Kubernetes cluster. This will allow for your cluster to access other compute resources within your VPC by name.  This gives you yet another option for service discovery using DNS.  Remember, you can also use Cloud Map as well.</p>

<p>While I did not show direct connectivity, I tested it and it works perfectly. One of the advantages of using EKS, is that all the pods use VPC IP addresses by default.  This makes IP connectivity within the VPC trivial.</p>

<hr />

<h3 id="references">References</h3>

<ul>
  <li><a href="https://coredns.io/">CoreDNS</a></li>
  <li><a href="https://kubernetes.io/blog/2018/07/10/coredns-ga-for-kubernetes-cluster-dns/">CoreDNS and Kubernetes</a></li>
  <li><a href="https://github.com/coredns/coredns/tree/master/plugin/route53">CoreDNS and Route53 plugin</a></li>
  <li><a href="https://coredns.io/explugins/amazondns/">CoreDNS and AmazonDNS</a></li>
  <li><a href="https://www.oreilly.com/library/view/learning-coredns/9781492047957/">CoreDNS book</a></li>
</ul>
:ET