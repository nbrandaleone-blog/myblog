I"‹=<p>One of the amazingly powerful aspects of Kubernetes is its dynamic nature. Deployments watch and replace failed pods, application versions can be rolled forward and backwards with ease, and both workers nodes and pods can be automatically scaled. This blog will discuss worker node scaling, known as <em>Cluster Autoscaling</em>.  We will discuss pod autoscaling using the <em>Horizontal Pod Autoscaler</em> in another blog <a href="http://www.nickaws.net/aws/2018/07/03/EKS-and-HPA.html">post</a>.</p>

<p>Most customers use the supplied CloudFormation script to create worker nodes for EKS.  This CF script automatically places the worker nodes into an AWS auto-scaling group (ASG). Now, metrics generated by this ASG are sent to CloudWatch Metrics by default. These metrics can be used to scale up/down the cluster in typical AWS fashion.  Unfortunately, these are the wrong metrics. The most common metric (CPU Utilization) is simply not appropriate for cluster scaling. It might be appropriate for Pod scaling, but <em>memory reservation</em> is the right metric for a cluster. Also, AWS does not collect memory stats on a per instance basis.  For that, you would need to install a monitoring agent, known as <a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/mon-scripts.html">CloudWatch Agent</a>. So, we have a choice, install the CW agent, and do things in AWS fashion, or turn to a Kubernetes solution.</p>

<p>Since we are running Kubernetes, the <em>right</em> answer is to use a K8 solution to this problem.  Also, the CloudWatch agent is looking at course memory status, which do not translate well into pod and cluster-wide memory stats. Interestingly enough, this type of <a href="https://docs.aws.amazon.com/AmazonECS/latest/developerguide/cloudwatch_alarm_autoscaling.html">statistic</a> <em>is</em> made available for ECS (Elastic Container Service), the AWS native container solution. Therefore, I would only adjust the kubernetes ASG size manually, and not rely on AWS ASG for its native polices for auto-scaling.</p>

<p><img src="/images/memory-stats.png" alt="ECS CloudWatch cluster metrics" /></p>

<h3 id="kubernetes-cluster-autoscaling">Kubernetes Cluster Autoscaling</h3>
<p>The <a href="https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler/cloudprovider/aws">documentation</a> for the <em>Cluster Autoscaler on AWS</em> is pretty good. Here are the steps to take:</p>
<ol>
  <li>The worker running the cluster autoscaler pod will need access to certain AWS resources and actions.  Since we may not take the time to set-up pod <a href="https://kubernetes.io/docs/concepts/configuration/assign-pod-node/">affinity</a> or <a href="https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/">taints</a>, we will simply apply an policy with the appropriate permissions to the ASG instance role. This way, any node running the autoscaler will have the right permissions.</li>
  <li>Edit the cluster template file, adjusting for local settings.
    <ul>
      <li>Update your region information</li>
      <li>Update the certificate authority path</li>
      <li>Update your Auto Scaling Group name, unless you are using ASG discovery</li>
      <li>If you are using kube2iam, you will have to add an annotation</li>
    </ul>
  </li>
  <li>When creating deployments, one should specify cpu and memory requirements so that the scheduler <em>knows</em> when to fail due to constraint limitations. This will trigger the autoscaler to create/delete a new node as necessary</li>
</ol>

<h3 id="cluster-autoscaling-issues">Cluster autoscaling issues</h3>
<p>There are several known issues with cluster autoscaling. For example:</p>
<ol>
  <li>cluster autoscaling will increase the size of the cluster once a pod fails to be deployed. That is, it is <strong><em>reactive</em></strong>, and it will take several minutes after a placement failure before the new worker node is ready and available.</li>
  <li>cluster autoscaling is <strong>NOT</strong> zone aware (for now)</li>
  <li>In EKS, one must run the autoscaler on a worker node. It should be run in the <em>kube-system</em> namespace, so it does not terminate the worker node it is running on.</li>
  <li>Cluster Autoscaler decreases the size of the cluster when some nodes are consistently unneeded for a significant amount of time. A node is unneeded when it has low utilization and all of its important pods can be moved elsewhere.</li>
  <li>Pods are given at most 10 minute to gracefully terminate, if the autoscaler decided to kill a node.</li>
</ol>

<h4 id="iam-role">IAM Role</h4>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="o">{</span>
    <span class="s2">"Version"</span>: <span class="s2">"2012-10-17"</span>,
    <span class="s2">"Statement"</span>: <span class="o">[</span>
        <span class="o">{</span>
            <span class="s2">"Effect"</span>: <span class="s2">"Allow"</span>,
            <span class="s2">"Action"</span>: <span class="o">[</span>
                <span class="s2">"autoscaling:DescribeAutoScalingGroups"</span>,
                <span class="s2">"autoscaling:DescribeAutoScalingInstances"</span>,
                <span class="s2">"autoscaling:DescribeTags"</span>,
                <span class="s2">"autoscaling:SetDesiredCapacity"</span>,
                <span class="s2">"autoscaling:TerminateInstanceInAutoScalingGroup"</span>
            <span class="o">]</span>,
            <span class="s2">"Resource"</span>: <span class="s2">"*"</span>
        <span class="o">}</span>
    <span class="o">]</span>
<span class="o">}</span></code></pre></figure>

<h4 id="autoscaler-template">Autoscaler template</h4>
<p>Other <a href="https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler/cloudprovider/aws/examples">templates</a> can be found in the repo.</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nt">---</span>
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    k8s-addon: cluster-autoscaler.addons.k8s.io
    k8s-app: cluster-autoscaler
  name: cluster-autoscaler
  namespace: kube-system
<span class="nt">---</span>
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRole
metadata:
  name: cluster-autoscaler
  labels:
    k8s-addon: cluster-autoscaler.addons.k8s.io
    k8s-app: cluster-autoscaler
rules:
- apiGroups: <span class="o">[</span><span class="s2">""</span><span class="o">]</span>
  resources: <span class="o">[</span><span class="s2">"events"</span>,<span class="s2">"endpoints"</span><span class="o">]</span>
  verbs: <span class="o">[</span><span class="s2">"create"</span>, <span class="s2">"patch"</span><span class="o">]</span>
- apiGroups: <span class="o">[</span><span class="s2">""</span><span class="o">]</span>
  resources: <span class="o">[</span><span class="s2">"pods/eviction"</span><span class="o">]</span>
  verbs: <span class="o">[</span><span class="s2">"create"</span><span class="o">]</span>
- apiGroups: <span class="o">[</span><span class="s2">""</span><span class="o">]</span>
  resources: <span class="o">[</span><span class="s2">"pods/status"</span><span class="o">]</span>
  verbs: <span class="o">[</span><span class="s2">"update"</span><span class="o">]</span>
- apiGroups: <span class="o">[</span><span class="s2">""</span><span class="o">]</span>
  resources: <span class="o">[</span><span class="s2">"endpoints"</span><span class="o">]</span>
  resourceNames: <span class="o">[</span><span class="s2">"cluster-autoscaler"</span><span class="o">]</span>
  verbs: <span class="o">[</span><span class="s2">"get"</span>,<span class="s2">"update"</span><span class="o">]</span>
- apiGroups: <span class="o">[</span><span class="s2">""</span><span class="o">]</span>
  resources: <span class="o">[</span><span class="s2">"nodes"</span><span class="o">]</span>
  verbs: <span class="o">[</span><span class="s2">"watch"</span>,<span class="s2">"list"</span>,<span class="s2">"get"</span>,<span class="s2">"update"</span><span class="o">]</span>
- apiGroups: <span class="o">[</span><span class="s2">""</span><span class="o">]</span>
  resources: <span class="o">[</span><span class="s2">"pods"</span>,<span class="s2">"services"</span>,<span class="s2">"replicationcontrollers"</span>,<span class="s2">"persistentvolumeclaims"</span>,<span class="s2">"persistentvolumes"</span><span class="o">]</span>
  verbs: <span class="o">[</span><span class="s2">"watch"</span>,<span class="s2">"list"</span>,<span class="s2">"get"</span><span class="o">]</span>
- apiGroups: <span class="o">[</span><span class="s2">"extensions"</span><span class="o">]</span>
  resources: <span class="o">[</span><span class="s2">"replicasets"</span>,<span class="s2">"daemonsets"</span><span class="o">]</span>
  verbs: <span class="o">[</span><span class="s2">"watch"</span>,<span class="s2">"list"</span>,<span class="s2">"get"</span><span class="o">]</span>
- apiGroups: <span class="o">[</span><span class="s2">"policy"</span><span class="o">]</span>
  resources: <span class="o">[</span><span class="s2">"poddisruptionbudgets"</span><span class="o">]</span>
  verbs: <span class="o">[</span><span class="s2">"watch"</span>,<span class="s2">"list"</span><span class="o">]</span>
- apiGroups: <span class="o">[</span><span class="s2">"apps"</span><span class="o">]</span>
  resources: <span class="o">[</span><span class="s2">"statefulsets"</span><span class="o">]</span>
  verbs: <span class="o">[</span><span class="s2">"watch"</span>,<span class="s2">"list"</span>,<span class="s2">"get"</span><span class="o">]</span>
- apiGroups: <span class="o">[</span><span class="s2">"storage.k8s.io"</span><span class="o">]</span>
  resources: <span class="o">[</span><span class="s2">"storageclasses"</span><span class="o">]</span>
  verbs: <span class="o">[</span><span class="s2">"watch"</span>,<span class="s2">"list"</span>,<span class="s2">"get"</span><span class="o">]</span>

<span class="nt">---</span>
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: Role
metadata:
  name: cluster-autoscaler
  namespace: kube-system
  labels:
    k8s-addon: cluster-autoscaler.addons.k8s.io
    k8s-app: cluster-autoscaler
rules:
- apiGroups: <span class="o">[</span><span class="s2">""</span><span class="o">]</span>
  resources: <span class="o">[</span><span class="s2">"configmaps"</span><span class="o">]</span>
  verbs: <span class="o">[</span><span class="s2">"create"</span><span class="o">]</span>
- apiGroups: <span class="o">[</span><span class="s2">""</span><span class="o">]</span>
  resources: <span class="o">[</span><span class="s2">"configmaps"</span><span class="o">]</span>
  resourceNames: <span class="o">[</span><span class="s2">"cluster-autoscaler-status"</span><span class="o">]</span>
  verbs: <span class="o">[</span><span class="s2">"delete"</span>,<span class="s2">"get"</span>,<span class="s2">"update"</span><span class="o">]</span>

<span class="nt">---</span>
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  name: cluster-autoscaler
  labels:
    k8s-addon: cluster-autoscaler.addons.k8s.io
    k8s-app: cluster-autoscaler
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-autoscaler
subjects:
  - kind: ServiceAccount
    name: cluster-autoscaler
    namespace: kube-system

<span class="nt">---</span>
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: RoleBinding
metadata:
  name: cluster-autoscaler
  namespace: kube-system
  labels:
    k8s-addon: cluster-autoscaler.addons.k8s.io
    k8s-app: cluster-autoscaler
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: cluster-autoscaler
subjects:
  - kind: ServiceAccount
    name: cluster-autoscaler
    namespace: kube-system

<span class="nt">---</span>
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: cluster-autoscaler
  namespace: kube-system
  labels:
    app: cluster-autoscaler
spec:
  replicas: 1
  selector:
    matchLabels:
      app: cluster-autoscaler
  template:
    metadata:
      labels:
        app: cluster-autoscaler
    spec:
      serviceAccountName: cluster-autoscaler
      containers:
        - image: k8s.gcr.io/cluster-autoscaler:v1.2.2
          name: cluster-autoscaler
          resources:
            limits:
              cpu: 100m
              memory: 300Mi
            requests:
              cpu: 100m
              memory: 300Mi
          <span class="nb">command</span>:
            - ./cluster-autoscaler
            - <span class="nt">--v</span><span class="o">=</span>4
            - <span class="nt">--stderrthreshold</span><span class="o">=</span>info
            - <span class="nt">--cloud-provider</span><span class="o">=</span>aws
            - <span class="nt">--skip-nodes-with-local-storage</span><span class="o">=</span><span class="nb">false</span>
            - <span class="nt">--nodes</span><span class="o">=</span>1:10:&lt;UPDATE WITH YOUR ASG NAME&gt;
          <span class="nb">env</span>:
            - name: AWS_REGION
              value: us-west-2
          volumeMounts:
            - name: ssl-certs
              mountPath: /etc/ssl/certs/ca-certificates.crt
              readOnly: <span class="nb">true
          </span>imagePullPolicy: <span class="s2">"Always"</span>
      volumes:
        - name: ssl-certs
          hostPath:
            path: <span class="s2">"/etc/ssl/certs/ca-bundle.crt"</span>
<span class="nt">---</span></code></pre></figure>

<h3 id="testing">Testing</h3>
<p>In order to test the autoscaler, I simply created a deployment and scaled it up until I <em>broke</em> my cluster.  After examining the logs of the cluster-autoscaler pod, I noticed this:</p>

<p>[scale_up.go:199] Best option to resize: eks-worker-nodes-NodeGroup-17QA3RV58XDBW<br />
  [scale_up.go:203] Estimated 1 nodes needed in eks-worker-nodes-NodeGroup-17QA3RV58XDBW<br />
  [scale_up.go:292] Final scale-up plan: [{eks-worker-nodes-NodeGroup-17QA3RV58XDBW 3-&gt;4 (max: 10)}]<br />
  [scale_up.go:344] Scale-up: setting group eks-worker-nodes-NodeGroup-17QA3RV58XDBW size to 4<br />
  [aws_manager.go:305] Setting asg eks-worker-nodes-NodeGroup-17QA3RV58XDBW size to 4</p>

<h3 id="summary">Summary</h3>
<p>I was able to confirm on my Console that my kubernets cluster increased in size from 3 to 4 nodes.  After killing off the deployment and waiting 10 minutes, the cluster shrank back to 3 nodes.  <strong>Success!</strong></p>

<hr />
<h4 id="references"><strong>References:</strong></h4>

<p><a href="https://kubernetes.io/blog/2016/07/autoscaling-in-kubernetes/">autoscaling in kubernetes</a></p>

<p><a href="https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler/cloudprovider/aws">cluster autoscaling</a></p>

<p><a href="https://akomljen.com/kubernetes-cluster-autoscaling-on-aws/">cluster autoscaling blog</a></p>

<p><a href="https://github.com/kubernetes/charts/tree/master/stable/cluster-autoscaler">Helm chart</a></p>

<p><a href="https://github.com/kubernetes/autoscaler/issues/1015">EKS autoscaling error</a></p>

<p><a href="https://github.com/pusher/k8s-spot-rescheduler">Kubernetes Rescheduler</a> or <a href="https://github.com/kubernetes/charts/tree/master/stable/k8s-spot-rescheduler">helm</a></p>

<p><a href="https://akomljen.com/meet-a-kubernetes-descheduler/">Kubernetes Descheduler</a></p>

<p><a href="https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/FAQ.md#when-does-cluster-autoscaler-change-the-size-of-a-cluster">FAQ</a></p>

<p><a href="https://docs.aws.amazon.com/eks/latest/userguide/what-is-eks.html">EKS documentation - html</a> or
 <a href="https://docs.aws.amazon.com/eks/latest/userguide/eks-ug.pdf">EKS documentation - PDF</a></p>
:ET